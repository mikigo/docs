"use strict";(self.webpackChunk=self.webpackChunk||[]).push([["4839"],{6693:function(e,s,n){n.r(s),n.d(s,{default:()=>l});var r=n(2676),i=n(453);function d(e){let s=Object.assign({h1:"h1",a:"a",h2:"h2",p:"p",ul:"ul",li:"li",pre:"pre",code:"code",img:"img",h3:"h3",h4:"h4"},(0,i.ah)(),e.components);return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(s.h1,{id:"scrapy",children:["Scrapy",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#scrapy",children:"#"})]}),"\n",(0,r.jsxs)(s.h2,{id:"1简介",children:["1、简介",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#1简介",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"Scrapy 是现阶段 Python 社区最流行的爬虫框架，它能够极大的简化爬虫的编写难度，简化代码。"}),"\n",(0,r.jsx)(s.p,{children:"当然它不是 Python 社区唯一的爬虫框架，但我认为是现阶段最好用的爬虫框架。"}),"\n",(0,r.jsx)(s.p,{children:"经常用同学问，为啥要用 Scrapy，我用 requests 不可以吗？"}),"\n",(0,r.jsx)(s.p,{children:"我觉得这样解释："}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"不是一个类型"}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"requests 最多算是爬虫工具，不同的人写出来的爬虫代码都不同，重复代码还多，而且对于一些高级的应用场景，如：多线程处理、异步处理、持久化等，估计没几个人能处理的很完美，最后爬下来的数据还要找一堆工具来解析处理，比如：re、BeautifulSoup、lxml等，属实让人挠头；"}),"\n",(0,r.jsx)(s.p,{children:"而爬虫框架通常提供了简单的配置，使用很少的代码就能实现复杂的功能，代码量少了，而且底层也为你处理了很对问题，框架在解析数据也有自带的方案，所以你只需要按照框架所定义好的规范，就可以轻松完成各种任务；"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"不是一个圈子"}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"Scrapy 主要用于数据爬取，所以说它是爬虫框架，你说用它来做一些 POST 请求发个数据啥的，咱们貌似没这么用过；"}),"\n",(0,r.jsx)(s.p,{children:"而 requests 只要是网络接口请求都能用，爬数据也可以，但你要说爬数据有多强呢，就要看使用的人有多强了；"}),"\n",(0,r.jsx)(s.p,{children:"总结："}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"新手、老司机做小任务，用哪个都无所谓，用框架的话会轻松很多；"}),"\n",(0,r.jsx)(s.li,{children:"新手做大任务，用框架，不要想，省时省力；"}),"\n",(0,r.jsx)(s.li,{children:"老司机做大任务，用工具可以做，就是有点麻烦；用框架也能搞，但是不能秀出你的实力；"}),"\n"]}),"\n",(0,r.jsxs)(s.h2,{id:"2安装",children:["2、安装",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#2安装",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"系统环境：deepin"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"sudo pip3 install Scrapy\n\n"})}),"\n",(0,r.jsxs)(s.h2,{id:"3创建项目",children:["3、创建项目",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#3创建项目",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"咱们就爬取 deepin 论坛的贴子，找找感觉；"}),"\n",(0,r.jsx)(s.p,{children:"创建一个爬虫项目名为：deepin_bbs_spider"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"cd ~\nscrapy startproject deepin_bbs_spider\n"})}),"\n",(0,r.jsx)(s.p,{children:"工程目录结构："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"deepin_bbs_spider\n├── deepin_bbs_spider\n│\xa0\xa0 ├── __init__.py\n│\xa0\xa0 ├── items.py  # 数据类型定义\n│\xa0\xa0 ├── middlewares.py  # 中间件\n│\xa0\xa0 ├── pipelines.py  # 数据管道\n│\xa0\xa0 ├── settings.py  # 配置项\n│\xa0\xa0 └── spiders  # 放爬虫脚本的目录\n│\xa0\xa0     └── __init__.py\n└── scrapy.cfg  # 部署配置文件\n"})}),"\n",(0,r.jsxs)(s.h2,{id:"4开始写爬虫",children:["4、开始写爬虫",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#4开始写爬虫",children:"#"})]}),"\n",(0,r.jsxs)(s.p,{children:["在 ",(0,r.jsx)(s.code,{children:"~/deepin_bbs_spider/deepin_bbs_spider/spiders"})," 目录下写我们的爬虫脚本文件，创建一个爬虫，目标是爬取论坛里面帖子内容："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'# bbs_spider.py\n\nimport scrapy\n\nclass BbsSpiderSpider(scrapy.Spider):\n    name = "bbs_spider"\n    allowed_domains = ["bbs.deepin.org"]\n    start_urls = ["https://bbs.deepin.org/?offset=0&limit=20&order=updated_at&where=&languages=zh_CN#comment_title"]\n\n    def parse(self, response):\n        post_items = response.css("app-main-pc > div > div:nth-child(3) > app-post-pc")\n        for post_item in post_items:\n            url = post_item.css("a.post_lin_pc::attr(href)").get()\n            title = post_item.css("span.ng-star-inserted::text").getall()\n            print("url:", url)\n            print("title", title)\n'})}),"\n",(0,r.jsx)(s.p,{children:"啥也不说，先跑起来试试："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"cd ~/deepin_bbs_spider\nscrapy crawl bbs_spider\n"})}),"\n",(0,r.jsx)(s.p,{children:"跑完之后，终端就会有输出爬取到的帖子信息："}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/bbs_show.png",alt:""})}),"\n",(0,r.jsx)(s.p,{children:"你先别管其他的，至少咱们能爬到数据了，接下来咱们慢慢介绍上面这些代码是怎么来的~；"}),"\n",(0,r.jsxs)(s.h2,{id:"5逻辑讲解",children:["5、逻辑讲解",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#5逻辑讲解",children:"#"})]}),"\n",(0,r.jsxs)(s.h3,{id:"51生成爬虫模板",children:["5.1、生成爬虫模板",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#51生成爬虫模板",children:"#"})]}),"\n",(0,r.jsxs)(s.p,{children:["看了上面的示例，有同学肯定要问，你咋知道要写个类呢，你咋知道要写个 ",(0,r.jsx)(s.code,{children:"parse"})," 函数呢？"]}),"\n",(0,r.jsxs)(s.p,{children:["我确实不知道，",(0,r.jsx)(s.code,{children:"scrapy"})," 也知道咱们不知道，所以做了个工具自动生成："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"scrapy genspider <spider name> <spider url>\n"})}),"\n",(0,r.jsxs)(s.p,{children:["用子命令 ",(0,r.jsx)(s.code,{children:"genspider"}),"，后面加爬虫的名称（spider name），再加要爬取地址（url），就可以在 ",(0,r.jsx)(s.code,{children:"spiders"})," 目录下自动生成一个 ",(0,r.jsx)(s.code,{children:"py"})," 文件；"]}),"\n",(0,r.jsx)(s.p,{children:"比如，咱们像这样："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:'scrapy genspider bbs_spider "https://bbs.deepin.org"\n'})}),"\n",(0,r.jsxs)(s.p,{children:["执行之后就会自动生成 ",(0,r.jsx)(s.code,{children:"py"})," 文件："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'import scrapy\n\nclass BbsSpiderSpider(scrapy.Spider):\n    name = "bbs_spider"\n    allowed_domains = ["bbs.deepin.org"]\n    start_urls = ["https://bbs.deepin.org"]\n\n    def parse(self, response):\n		pass\n'})}),"\n",(0,r.jsx)(s.p,{children:"简单讲解一下："}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["爬虫类是要继承 ",(0,r.jsx)(s.code,{children:"scrapy.Spider"})," 的，这个不要去动，知道继承就对了；"]}),"\n",(0,r.jsxs)(s.li,{children:["类变量 ",(0,r.jsx)(s.code,{children:"name"})," 是爬虫的名称，这玩意儿就是个代号，你想改成王大锤都行，一般赖得去管；"]}),"\n",(0,r.jsxs)(s.li,{children:["类变量 ",(0,r.jsx)(s.code,{children:"allowed_domains"})," 爬虫域名；"]}),"\n",(0,r.jsxs)(s.li,{children:["类变量 ",(0,r.jsx)(s.code,{children:"start_urls"})," 爬虫目标地址，可以给多个；"]}),"\n",(0,r.jsxs)(s.li,{children:["实例方法 ",(0,r.jsx)(s.code,{children:"parse(self, response)"})," 也是固定写法，函数名称最好不动，参数名称不能改，因为是 scrapy 返回的一个 Selector 对象；"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:["这里面核心逻辑就是在 ",(0,r.jsx)(s.code,{children:"parse"})," 函数里面去写，你可以理解成 ",(0,r.jsx)(s.code,{children:"response"})," 就是返回的页面信息，你只需要在这里面去提取想要的数据就好了；"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.code,{children:"response"})," 提供一些方法，能够很方便的进行页面信息提取；"]}),"\n",(0,r.jsxs)(s.h3,{id:"52爬虫编写方法",children:["5.2、爬虫编写方法",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#52爬虫编写方法",children:"#"})]}),"\n",(0,r.jsxs)(s.p,{children:["前面说到爬虫脚本里面 ",(0,r.jsx)(s.code,{children:"response"}),"，它是我们编写代码的核心，所有的数据提取都从这里来；"]}),"\n",(0,r.jsx)(s.p,{children:"下面我们讲讲数据的提取方法，这里多嘴一句，我默认大家都是有一点前端基础的，不然下面部分内容可能需要去学习下 html、css相关知识；"}),"\n",(0,r.jsx)(s.p,{children:"首先来讲 css 提取方法，css 的解析是非常灵活的，先用 F12 看下 html 源码："}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/html.png",alt:""})}),"\n",(0,r.jsxs)(s.p,{children:["可以看到所有的帖子都在 ",(0,r.jsx)(s.code,{children:"app-post-pc"})," 标签下面，咱们可以这样写："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'def parse(self, response):\n    post_items = response.css("app-post-pc")\n'})}),"\n",(0,r.jsx)(s.p,{children:"如果你是使用右键复制的选择器，可能是一个很长的表达式，不太优雅也不利于维护，我个人不太建议使用直接复制表达式，而应该通过观察自己写；"}),"\n",(0,r.jsxs)(s.p,{children:["这样的话，",(0,r.jsx)(s.code,{children:"post_items"})," 就获取到了所有帖子的 ",(0,r.jsx)(s.code,{children:"app-post-pc"})," 标签，再看下 ",(0,r.jsx)(s.code,{children:"app-post-pc"})," 标签下都有啥："]}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/a_tag.png",alt:""})}),"\n",(0,r.jsxs)(s.p,{children:["可以看到在 ",(0,r.jsx)(s.code,{children:"app-post-pc"})," 标签下还有 a 标签，保存了帖子的详情地址(",(0,r.jsx)(s.code,{children:"post_url"}),")，然后在 a 标签下的 span 标签保存了帖子的类型和标题(",(0,r.jsx)(s.code,{children:"title"}),")，因此咱们想办法把这两个拿到："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'def parse(self, response):\n    post_items = response.css("app-post-pc")\n    for post_item in post_items:\n        url = post_item.css("a.post_lin_pc::attr(href)").get()\n        title = post_item.css("span.ng-star-inserted::text").getall()\n        print("url:", url)\n        print("title", title)\n'})}),"\n",(0,r.jsxs)(s.p,{children:["先用 for 循环把 ",(0,r.jsx)(s.code,{children:"post_items"})," 里面每个 ",(0,r.jsx)(s.code,{children:"Selector"})," 对象里面的 ",(0,r.jsx)(s.code,{children:"url"})," 和 ",(0,r.jsx)(s.code,{children:"title"})," 拿到；"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.code,{children:"post_item"})," 就是单个的 ",(0,r.jsx)(s.code,{children:"Selector"})," 对象，我们在它的基础上再通过 css 方法获取到我们想要的数据；（也可以使用 Xpath 技术获取）"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"url"})," 是在 a 标签里面的 href 属性里面，因此："]}),"\n"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'post_item.css("a.post_lin_pc::attr(href)").get()\n'})}),"\n",(0,r.jsxs)(s.p,{children:["表达式里面的 ",(0,r.jsx)(s.code,{children:"::attr(href)"})," 这部分是 Scrapy 特有的，",(0,r.jsx)(s.code,{children:"::"})," 表示取值，",(0,r.jsx)(s.code,{children:"attr(href)"})," 表示通过 ",(0,r.jsx)(s.code,{children:"href"})," 属性取值；"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.code,{children:"get()"})," 方法表示取第一个值，",(0,r.jsx)(s.code,{children:"getall()"})," 方法表示取所有的值；（也兼容老版本的 ",(0,r.jsx)(s.code,{children:"extract_first()"})," 和 ",(0,r.jsx)(s.code,{children:"extract()"})," 方法，意思是对应一样的，不过明显",(0,r.jsx)(s.code,{children:"get()"})," 这种可读性更好更易于理解。）"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"title"})," 在 span 标签里面："]}),"\n"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'post_item.css("span.ng-star-inserted::text").getall()\n'})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.code,{children:"text"})," 也是 Scrapy 特有的，表示把标签的文本取出来；"]}),"\n",(0,r.jsx)(s.p,{children:"非常好理解对吧，只要你稍微有点前端知识，就能够轻松把表达式写出来；"}),"\n",(0,r.jsxs)(s.h3,{id:"53获取数据",children:["5.3、获取数据",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#53获取数据",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"前面例子是将获取到的数据打印出来，实际业务里面我们肯定是需要将数据保存下来的；"}),"\n",(0,r.jsxs)(s.p,{children:["首先我们在 ",(0,r.jsx)(s.code,{children:"items.py"})," 里面定义数据类型："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:"# items.py\n\nimport scrapy\n\nclass DeepinBbsSpiderItem(scrapy.Item):\n    # define the fields for your item here like:\n    url = scrapy.Field()\n    title = scrapy.Field()\n"})}),"\n",(0,r.jsxs)(s.p,{children:["写法非常简单，统一使用 ",(0,r.jsx)(s.code,{children:"scrapy.Field()"})," 来定义就行了；"]}),"\n",(0,r.jsx)(s.p,{children:"然后，回到爬虫脚本里面："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'import scrapy\nfrom deepin_bbs_spider.items import DeepinBbsSpiderItem\n\nclass BbsSpiderSpider(scrapy.Spider):\n    ... # 省略部分代码\n\n    def parse(self, response):\n        item = DeepinBbsSpiderItem()\n        post_items = response.css("app-post-pc")\n        for post_item in post_items:\n            item["url"] = post_item.css("a.post_lin_pc::attr(href)").get()\n            item["title"] = post_item.css("span.ng-star-inserted::text").getall()\n            yield item\n'})}),"\n",(0,r.jsxs)(s.p,{children:["将 ",(0,r.jsx)(s.code,{children:"items.py"})," 里面的 ",(0,r.jsx)(s.code,{children:"DeepinBbsSpiderItem"})," 导进来，实例化一个对象，然后将获取到的数据复制给这个对象，使用 ",(0,r.jsx)(s.code,{children:'item["url"]'})," 这种给字典添加的方式，注意要和 ",(0,r.jsx)(s.code,{children:"items.py"})," 里面定义的字段名称保持一致；"]}),"\n",(0,r.jsxs)(s.p,{children:["最后，使用 ",(0,r.jsx)(s.code,{children:"yield"})," 将数据返回出来就行了；"]}),"\n",(0,r.jsx)(s.p,{children:"将数据写入到 csv 文件里面："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"scrapy crawl bbs_spider -o bbs.csv\n"})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.code,{children:"-o"})," 表示导出数据，执行后，查看 bbs.csv 文件："]}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/csv.png",alt:""})}),"\n",(0,r.jsx)(s.p,{children:"这样就将爬取到的数据保存到了一个 csv 文件；"}),"\n",(0,r.jsxs)(s.h3,{id:"54处理数据",children:["5.4、处理数据",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#54处理数据",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"在爬虫脚本里面获取到原始数据之后，我们还有可能会拿数据做进一步处理，比如还想写入数据库、写入 Excel等等；"}),"\n",(0,r.jsxs)(s.p,{children:["这些进一步的操作，我们通常是在数据管道 ",(0,r.jsx)(s.code,{children:"pipelines.py"})," 里面来处理："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:"class DeepinBbsSpiderPipeline:\n    def process_item(self, item, spider):\n        return item\n"})}),"\n",(0,r.jsxs)(s.p,{children:["这里的 ",(0,r.jsx)(s.code,{children:"item"})," 就是每一条数据；"]}),"\n",(0,r.jsxs)(s.p,{children:["比如，你想写入 ",(0,r.jsx)(s.code,{children:" MySQL"}),"数据库（首先要确保数据库表、字段等正常）："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:"import pymysql\n\nclass DeepinBbsSpiderPipeline:\n\n    def __init__(self):\n        # 在构造函数里面创建数据库连接和游标\n    \n    def open_spider(self, spider):\n        # open_spider 是这个管道开始时要执行的；这里可以不要\n    \n    def close_spider(self, spider):\n        # close_spider 写入关闭数据库的代码\n    \n    def process_item(self, item, spider):\n        # 在这里做写入数据库的动作\n        return item \n"})}),"\n",(0,r.jsxs)(s.p,{children:["在上面注释里面写了写入数据库的编写逻辑，由于我们主要想讲解数据管道的操作逻辑，数据库的代码数据基本操作，就不做详细代码示例了，往上搜 ",(0,r.jsx)(s.code,{children:"pymysql"})," 的使用很多，按照注释的逻辑，对号入座就行了；"]}),"\n",(0,r.jsxs)(s.p,{children:["如果想写入 Excel 表格逻辑是一样的，也可以表格和数据库同时写入，在 ",(0,r.jsx)(s.code,{children:"pipelines.py"})," 里面再定义一个管道类就行了；"]}),"\n",(0,r.jsxs)(s.p,{children:["注意，数据管道逻辑写完之后，要在 ",(0,r.jsx)(s.code,{children:"settings.py"})," 里面修改配置："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'ITEM_PIPELINES = {\n    "deepin_bbs_spider.pipelines.DeepinBbsSpiderPipeline": 300,\n    # "deepin_bbs_spider.pipelines.XxxxPipeline": 2,\n}\n'})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.code,{children:"ITEM_PIPELINES"})," 是一个字典，key 是数据管道，value 是一个数字；"]}),"\n",(0,r.jsxs)(s.p,{children:["value 主要用于多个管道排序的，因为在 ",(0,r.jsx)(s.code,{children:"pipelines.py"})," 里面可以定义多个数据管道类，它们执行的先后顺序由 value 来控制，数字越小越先执行；"]}),"\n",(0,r.jsx)(s.p,{children:"如果你就一个数据管道类，这个 value 给多少都无所谓；"}),"\n",(0,r.jsxs)(s.p,{children:["另外提醒，在 ",(0,r.jsx)(s.code,{children:"process_item()"})," 最后一定要 ",(0,r.jsx)(s.code,{children:"return item "}),"，不然存在多个数据管道的时候，后执行的数据管道就拿不到数据了；"]}),"\n",(0,r.jsx)(s.p,{children:"好，配置完之后，就可以再次执行了；"}),"\n",(0,r.jsxs)(s.h3,{id:"55从下层页面解析数据",children:["5.5、从下层页面解析数据",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#55从下层页面解析数据",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"这部分内容相对来讲是难点，搞懂了这部分，就几乎能处理对大部分数据爬取了；"}),"\n",(0,r.jsx)(s.p,{children:"来，开始燥起来~~"}),"\n",(0,r.jsxs)(s.p,{children:["前面我们获取到了帖子的 ",(0,r.jsx)(s.code,{children:"url"})," 和 ",(0,r.jsx)(s.code,{children:"title"}),"，有同学可能要问了，这个帖子的正文内容哪里；"]}),"\n",(0,r.jsxs)(s.p,{children:["正文内容在帖子的 ",(0,r.jsx)(s.code,{children:"url"})," 里面，如果我们要同时获取帖子的正文内容，就需要做以下处理；"]}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/detail.png",alt:""})}),"\n",(0,r.jsxs)(s.h4,{id:"551回调逻辑",children:["5.5.1、回调逻辑",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#551回调逻辑",children:"#"})]}),"\n",(0,r.jsxs)(s.p,{children:["首先，前面获取的 ",(0,r.jsx)(s.code,{children:"url"})," 不是一个完整的链接，咱们需要稍微处理以下："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'class BbsSpiderSpider(scrapy.Spider):\n    \n    base_url = "https://bbs.deepin.org"\n\n    def parse(self, response):\n        item = DeepinBbsSpiderItem()\n        post_items = response.css("app-post-pc")\n        for post_item in post_items:\n            item["url"] = post_item.css("a.post_lin_pc::attr(href)").get().replace("/en", self.base_url)\n	# 省略部分代码\n'})}),"\n",(0,r.jsxs)(s.p,{children:["我们前面获取的 ",(0,r.jsx)(s.code,{children:"url"})," 是这样的: ",(0,r.jsx)(s.code,{children:"/en/post/254787"})," ，因此做一个替换处理；"]}),"\n",(0,r.jsxs)(s.p,{children:["然后，咱们拿着这个 ",(0,r.jsx)(s.code,{children:"url"})," 继续做请求："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'class BbsSpiderSpider(scrapy.Spider):\n    \n    base_url = "https://bbs.deepin.org"\n\n    def parse(self, response):\n        item = DeepinBbsSpiderItem()\n        post_items = response.css("app-post-pc")\n        for post_item in post_items:\n            item["url"] = post_item.css("a.post_lin_pc::attr(href)").get().replace("/en", self.base_url)\n            yield scrapy.Request(\n                url=item["url"], \n                callback=self.post_parse, \n                cb_kwargs={"item": item}\n            )\n\n    def post_parse(self, response, **kwargs):\n        item = kwargs.get("item")\n'})}),"\n",(0,r.jsxs)(s.p,{children:["这里需要用 ",(0,r.jsx)(s.code,{children:"yield"})," 返回并构造 scrapy.Request 对象，传入三个参数："]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsx)(s.p,{children:"url 就是下层页面的地址；"}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:["callback 传入回调函数对象，因为 ",(0,r.jsx)(s.code,{children:"parse()"})," 这个函数是处理当前页面的逻辑，下层页面就不能在这个函数里面继续处理了，而是要新写一个函数来处理；"]}),"\n",(0,r.jsxs)(s.p,{children:["写法和 ",(0,r.jsx)(s.code,{children:"parse()"})," 类似，函数名可以自己定， 参数仍然是 response 对象；"]}),"\n",(0,r.jsxs)(s.p,{children:["注意，参数传入是 ",(0,r.jsx)(s.code,{children:"callback=self.post_parse"}),"，后面没有加括号哈，因为我们不是在这里调用函数，是传入函数对象，也就是只要函数名；"]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:["cb_kwargs 是为了给 ",(0,r.jsx)(s.code,{children:"post_parse()"})," 函数传递 item 参数，是一个字典类型，这样在 ",(0,r.jsx)(s.code,{children:"post_parse(self, response, **kwargs)"})," 里面的 ",(0,r.jsx)(s.code,{children:"kwargs"})," 就能拿到 item 的值，咱们后续拿到正文之后，继续组装到 item 里面就行了；"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(s.h4,{id:"552下层页面解析",children:["5.5.2、下层页面解析",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#552下层页面解析",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"下层页面的解析，逻辑和前面一样，先看下 html 源码："}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/post_info.png",alt:""})}),"\n",(0,r.jsx)(s.p,{children:"获取正文："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'    def post_parse(self, response, **kwargs):\n        item = kwargs.get("item")\n        post_info = response.css("div.post_conten > div.post_edit.ng-star-inserted > div > div > p::text").getall()\n'})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.code,{children:"post_info"})," 获取的结果为："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"['1、系统盘分配了40g,这才一个月就快满了，怎么调大点，后面还有100G空间。', '2、应用商店啥时候放出conky？']\n"})}),"\n",(0,r.jsx)(s.p,{children:"做一个字符串组装："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'post_info = "".join(response.css("div.post_conten > div.post_edit.ng-star-inserted > div > div > p::text").getall())\n'})}),"\n",(0,r.jsx)(s.p,{children:"这样的话，我们就获取到了正文的数据，添加到 item 对象中："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'def post_parse(self, response, **kwargs):\n    item = kwargs.get("item")\n    post_info = "".join(response.css("div.post_conten > div.post_edit.ng-star-inserted > div > div > p::text").getall())\n    item["post_info"] = post_info\n    yield item\n'})}),"\n",(0,r.jsxs)(s.p,{children:["注意，在 ",(0,r.jsx)(s.code,{children:"items.py"})," 中把新的字段也添加上："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:"# items.py\n\nclass DeepinBbsSpiderItem(scrapy.Item):\n    ...\n    post_info = scrapy.Field()\n"})}),"\n",(0,r.jsx)(s.p,{children:"最后，跑一下爬虫；"}),"\n",(0,r.jsxs)(s.h4,{id:"553多层数据传递问题",children:["5.5.3、多层数据传递问题",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#553多层数据传递问题",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"到目前位置，完整的爬虫脚本："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'import scrapy\nfrom deepin_bbs_spider.items import DeepinBbsSpiderItem\n\nclass BbsSpiderSpider(scrapy.Spider):\n    name = "bbs_spider"\n    allowed_domains = ["bbs.deepin.org"]\n    start_urls = ["https://bbs.deepin.org/?offset=0&limit=20&order=updated_at&where=&languages=zh_CN#comment_title"]\n    base_url = "https://bbs.deepin.org"\n\n    def parse(self, response):\n        item = DeepinBbsSpiderItem()\n        post_items = response.css("app-main-pc > div > div:nth-child(3) > app-post-pc")\n        for post_item in post_items:\n            item["url"] = post_item.css("a.post_lin_pc::attr(href)").get().replace("/en", self.base_url)\n            item["title"] = "".join(post_item.css("span.ng-star-inserted::text").getall()[:2])\n            yield scrapy.Request(\n                url=item["url"],\n                callback=self.post_parse,\n                cb_kwargs={"item": item}\n            )\n\n    def post_parse(self, response, **kwargs):\n        item = kwargs.get("item")\n        post_info = "".join(response.css("div.post_conten > div.post_edit.ng-star-inserted > div > div > p::text").getall())\n        item["post_info"] = post_info\n        yield item\n'})}),"\n",(0,r.jsx)(s.p,{children:"使用命令跑一下爬虫："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"scrapy crawl bbs_spider -o bbs.csv\n"})}),"\n",(0,r.jsx)(s.p,{children:"你会惊奇的发现，怎么所有的 title 和 url 数据相同，开始怀疑自己逻辑是不是写错了；"}),"\n",(0,r.jsx)(s.p,{children:"其实，我们代码逻辑是没问题的，只不过在多层数据传递的过程中，需要特殊处理下，处理方法很简单："}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["导入",(0,r.jsx)(s.code,{children:"from copy import deepcopy"}),"模块，将",(0,r.jsx)(s.code,{children:"cb_kwargs={'item': item}"})," 更改为 ",(0,r.jsx)(s.code,{children:"cb_kwargs={'item': deepcopy(item)"}),"；"]}),"\n",(0,r.jsxs)(s.li,{children:["最后一行代码",(0,r.jsx)(s.code,{children:"yield item"})," 修改成 ",(0,r.jsx)(s.code,{children:"yield deepcopy(item)"}),"就完全 ",(0,r.jsx)(s.code,{children:"ok"})," 了；"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"改完之后再跑一下，简直完美。"}),"\n",(0,r.jsxs)(s.h4,{id:"554多页面爬取",children:["5.5.4、多页面爬取",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#554多页面爬取",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"到现在我们怕去了第一页的数据，那还想爬取后面的页怎么办？"}),"\n",(0,r.jsxs)(s.p,{children:["有同学说，好办，",(0,r.jsx)(s.code,{children:"start_urls"})," 不是一个列表吗，把多个 url 放进去不就完了；"]}),"\n",(0,r.jsx)(s.p,{children:"不得不说，这样是可以的，就是不够优雅。"}),"\n",(0,r.jsx)(s.p,{children:"通过仔细观察，我们可以发现一些规律："}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/urls.png",alt:""})}),"\n",(0,r.jsxs)(s.p,{children:["在地址中只有 ",(0,r.jsx)(s.code,{children:"offset"})," 参数在变化，第一页是 0，第二页是 1，非常有规律，因此咱们可以动态生成："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'class BbsSpiderSpider(scrapy.Spider):\n    # start_urls = ["https://bbs.deepin.org/?offset=0&limit=20&order=updated_at&where=&languages=zh_CN#comment_title"]\n\n    def start_requests(self):\n        for i in range(5):\n            yield scrapy.Request(url=f"https://bbs.deepin.org/?offset={i}&limit=20&order=updated_at&where=&languages=zh_CN#comment_title")\n'})}),"\n",(0,r.jsxs)(s.p,{children:["使用 ",(0,r.jsx)(s.code,{children:"start_requests()"})," 函数替代 ",(0,r.jsx)(s.code,{children:"start_urls"}),"；"]}),"\n",(0,r.jsxs)(s.p,{children:["在里面写个 for 循环，要爬取多少页填入 ",(0,r.jsx)(s.code,{children:"range"})," 函数就行了，动态生成多个 ",(0,r.jsx)(s.code,{children:"scrapy.Request"})," 对象，注意要用 yield 哦~~"]}),"\n",(0,r.jsxs)(s.h4,{id:"555完整的示例",children:["5.5.5、完整的示例",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#555完整的示例",children:"#"})]}),"\n",(0,r.jsxs)(s.p,{children:["爬虫脚本 ",(0,r.jsx)(s.code,{children:"bbs_spider.py"}),"："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'from copy import deepcopy\n\nimport scrapy\n\nfrom deepin_bbs_spider.items import DeepinBbsSpiderItem\n\n\nclass BbsSpiderSpider(scrapy.Spider):\n    name = "bbs_spider"\n    allowed_domains = ["bbs.deepin.org"]\n    base_url = "https://bbs.deepin.org"\n\n    def start_requests(self):\n        for i in range(5):\n            yield scrapy.Request(url=f"https://bbs.deepin.org/?offset={i}&limit=20&order=updated_at&where=&languages=zh_CN#comment_title")\n\n    def parse(self, response):\n        item = DeepinBbsSpiderItem()\n        post_items = response.css("app-main-pc > div > div:nth-child(3) > app-post-pc")\n        for post_item in post_items:\n            item["url"] = post_item.css("a.post_lin_pc::attr(href)").get().replace("/en", self.base_url)\n            item["title"] = "".join(post_item.css("span.ng-star-inserted::text").getall()[:2])\n            yield scrapy.Request(\n                url=item["url"],\n                callback=self.post_parse,\n                cb_kwargs={"item": deepcopy(item)}\n            )\n\n    def post_parse(self, response, **kwargs):\n        item = kwargs["item"]\n        post_info = "".join(\n            response.css("div.post_conten > div.post_edit.ng-star-inserted > div > div > p::text").getall()\n        )\n        item["post_info"] = post_info\n        yield deepcopy(item)\n'})}),"\n",(0,r.jsxs)(s.p,{children:["数据类型 ",(0,r.jsx)(s.code,{children:"items.py"}),"："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:"import scrapy\n\nclass DeepinBbsSpiderItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    url = scrapy.Field()\n    title = scrapy.Field()\n    post_info = scrapy.Field()\n"})}),"\n",(0,r.jsxs)(s.p,{children:["配置 ",(0,r.jsx)(s.code,{children:"settings.py"}),"：（省略了没有启用的配置项）"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'BOT_NAME = "deepin_bbs_spider"\n\nSPIDER_MODULES = ["deepin_bbs_spider.spiders"]\nNEWSPIDER_MODULE = "deepin_bbs_spider.spiders"\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\nUSER_AGENT = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n\n# Override the default request headers:\nDEFAULT_REQUEST_HEADERS = {\n   "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",\n   "Accept-Language": "en",\n}\n\n# Set settings whose default value is deprecated to a future-proof value\nREQUEST_FINGERPRINTER_IMPLEMENTATION = "2.7"\nTWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"\nFEED_EXPORT_ENCODING = "utf-8"\n'})}),"\n",(0,r.jsxs)(s.p,{children:["如果你自己玩起来有点小问题，可以尝试参考我的代码：",(0,r.jsx)(s.a,{href:"https://github.com/mikigo/deepin_bbs_spider",target:"_blank",rel:"noopener noreferrer",children:"https://github.com/mikigo/deepin_bbs_spider"})]}),"\n",(0,r.jsxs)(s.h2,{id:"6调试",children:["6、调试",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#6调试",children:"#"})]}),"\n",(0,r.jsxs)(s.h3,{id:"61数据获取调试",children:["6.1、数据获取调试",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#61数据获取调试",children:"#"})]}),"\n",(0,r.jsxs)(s.p,{children:["在使用 ",(0,r.jsx)(s.code,{children:"response.css()"})," 表达式时，通常我们需要进行调试，看表达式写得对不对，当然你可以通过执行爬虫然后打印数据，但是这种方式有点麻烦；"]}),"\n",(0,r.jsx)(s.p,{children:"Scrapy 提供了一个快捷的调试方法，在终端输入："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"scrapy shell <scrapy url>\n"})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.code,{children:"<scrapy url>"})," 是你要爬取的地址，比如前面我们想获取帖子正文的内容，可以这样调试："]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"scrapy shell https://bbs.deepin.org/post/254892\n"})}),"\n",(0,r.jsx)(s.p,{children:"进入终端交互式，输入："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:">>> response.css(\"div.post_conten > div.post_edit.ng-star-inserted > div > div > p::text\").getall()\n['1、系统盘分配了40g,这才一个月就快满了，怎么调大点，后面还有100G空间。', '2、应用商店啥时候放出conky？']\n"})}),"\n",(0,r.jsx)(s.p,{children:"可以看到返回的结果，如果返回为空，就说明表达式可能有点问题；"}),"\n",(0,r.jsxs)(s.h3,{id:"62pycharm-debug",children:["6.2、Pycharm Debug",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#62pycharm-debug",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"Scrapy 由于封装得比较好，启动爬虫是通过命令行启动，但是这有个问题，就是不支持在编辑器里面 Debug 运行，导致你调试代码过程中可能会不停的在终端启动爬虫，有点费劲；"}),"\n",(0,r.jsx)(s.p,{children:"经过一番折腾，终于道破了天机~"}),"\n",(0,r.jsxs)(s.p,{children:["（1）先在工程下随便找一个 ",(0,r.jsx)(s.code,{children:"py"})," 文件，里面啥也不写，执行一下，然后点这里："]}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/go_config.png",alt:""})}),"\n",(0,r.jsx)(s.p,{children:"（2）在系统中找到 scrapy 包中的 cmdline.py 文件，这个你得稍微知道点 Python 包管理的一些知识，比如我的在这里："}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-console",children:"/home/mikigo/.local/lib/python3.7/site-packages/scrapy/cmdline.py\n"})}),"\n",(0,r.jsxs)(s.p,{children:["（4）在 ",(0,r.jsx)(s.code,{children:"Name"})," 里面写个你喜欢的名字，比如我写：Scrapy"]}),"\n",(0,r.jsxs)(s.p,{children:["（4）在 ",(0,r.jsx)(s.code,{children:"Script path"})," 里面把 ",(0,r.jsx)(s.code,{children:"cmdline.py"})," 的路径填进去；"]}),"\n",(0,r.jsxs)(s.p,{children:["（5）在 ",(0,r.jsx)(s.code,{children:"Parameters"})," 里面填入 Scrapy 的参数：",(0,r.jsx)(s.code,{children:"crawl bbs_spider -o bbs.csv"}),"；"]}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/args_config.png",alt:""})}),"\n",(0,r.jsx)(s.p,{children:"（6）点击右下角的 【ok】，在主界面点【Debug】就可以进行调试了，妙啊~~"}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.img,{src:"/spider/debug.png",alt:""})}),"\n",(0,r.jsxs)(s.h2,{id:"7结束语",children:["7、结束语",(0,r.jsx)(s.a,{className:"header-anchor","aria-hidden":"true",href:"#7结束语",children:"#"})]}),"\n",(0,r.jsx)(s.p,{children:"到这里 Scrapy 的基础入门就结束了，一般的小网站可以轻松快速的搞定，简直 yyds~~"}),"\n",(0,r.jsx)(s.p,{children:"从完整示例我们不难看出，爬虫脚本简单的 30 来行代码加上简单的配置，就可以爬取大量的数据，而且速度非常快，对比你用 requests 去裸写看看，你会发现差距不是一般的大；"}),"\n",(0,r.jsx)(s.p,{children:"对于其他的一些细节还可以完善，比如：代理、异步、中间件、与其他自动化工具扩展（Selenium），后续精力再补充~~"})]})}function c(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:s}=Object.assign({},(0,i.ah)(),e.components);return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}let l=c;c.__RSPRESS_PAGE_META={},c.__RSPRESS_PAGE_META["program%2F%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%2FScrapy.md"]={toc:[{text:"1、简介",id:"1简介",depth:2},{text:"2、安装",id:"2安装",depth:2},{text:"3、创建项目",id:"3创建项目",depth:2},{text:"4、开始写爬虫",id:"4开始写爬虫",depth:2},{text:"5、逻辑讲解",id:"5逻辑讲解",depth:2},{text:"5.1、生成爬虫模板",id:"51生成爬虫模板",depth:3},{text:"5.2、爬虫编写方法",id:"52爬虫编写方法",depth:3},{text:"5.3、获取数据",id:"53获取数据",depth:3},{text:"5.4、处理数据",id:"54处理数据",depth:3},{text:"5.5、从下层页面解析数据",id:"55从下层页面解析数据",depth:3},{text:"5.5.1、回调逻辑",id:"551回调逻辑",depth:4},{text:"5.5.2、下层页面解析",id:"552下层页面解析",depth:4},{text:"5.5.3、多层数据传递问题",id:"553多层数据传递问题",depth:4},{text:"5.5.4、多页面爬取",id:"554多页面爬取",depth:4},{text:"5.5.5、完整的示例",id:"555完整的示例",depth:4},{text:"6、调试",id:"6调试",depth:2},{text:"6.1、数据获取调试",id:"61数据获取调试",depth:3},{text:"6.2、Pycharm Debug",id:"62pycharm-debug",depth:3},{text:"7、结束语",id:"7结束语",depth:2}],title:"Scrapy",headingTitle:"Scrapy",frontmatter:{Author:"mikigo"}}}}]);